{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Hey there, I'm Josh from (West) Seattle, Washington, USA! Welcome to my humble corner of the internet.</p> <p>I like computers. I like baking. You will mostly find things related to these topics here.</p> <p>Be kind to others, thanks for visiting.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/","title":"Setting Up Automated Backups with Restic on Debian Linux","text":"<p>This article was created out of a convo with Claude 3.5 Sonnet.</p> <p>Backups are crucial, but they need to be automated and reliable to be truly effective. In this guide, we'll set up a robust backup system using restic on Debian Linux 12. We'll configure it to back up Docker volumes (/var/lib/docker/volumes), home directories (/home), and system configuration files (/etc), with snapshots every 12 hours and a sensible retention policy. We'll use systemd to schedule our backups, easily audit backups, and manually run a backup job when desired.</p>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#prerequisites","title":"Prerequisites","text":"<ul> <li>Debian 12 (or similar Linux distribution)</li> <li>restic installed (<code>apt install restic</code>)</li> <li>root access or sudo privileges</li> <li>A backup destination (local disk, SFTP server, or S3 bucket)</li> </ul>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#the-setup-process","title":"The Setup Process","text":"<p>We'll create four files to handle our backup system:</p> <ol> <li>A backup script</li> <li>An environment file for credentials</li> <li>A systemd service</li> <li>A systemd timer</li> </ol> <p>Let's build this system piece by piece.</p>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#1-creating-the-backup-script","title":"1. Creating the Backup Script","text":"<p>Create a new file at <code>/usr/local/bin/backup-script.sh</code>:</p> <pre><code>#!/bin/bash\n\n# Exit on error\nset -e\n\n# Load environment variables\nsource /etc/restic-env\n\n# Paths to backup\nBACKUP_PATHS=\"/var/lib/docker/volumes \\\n             /home \\\n             /etc \\\n             /usr/local/etc\"\n\n# Initialize the repository if it doesn't exist\nrestic snapshots || restic init\n\n# Perform the backup\nrestic backup $BACKUP_PATHS\n\n# Keep the last 6 snapshots (72 hours worth, given 12-hour frequency)\n# Keep 1 snapshot per week for all other snapshots\nrestic forget --keep-last 6 --keep-weekly 1 --prune\n</code></pre> <p>Make the script executable: <code>sudo chmod +x /usr/local/bin/backup-script.sh</code></p>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#2-setting-up-the-environment-file","title":"2. Setting Up the Environment File","text":"<p>Create <code>/etc/restic-env</code> to store your credentials and configuration:</p> <pre><code># Repository location - adjust this to your needs\n# Examples:\n# export RESTIC_REPOSITORY=\"sftp:user@host:/srv/restic-repo\"\n# export RESTIC_REPOSITORY=\"s3:s3.amazonaws.com/bucket_name\"\n# export RESTIC_REPOSITORY=\"/path/to/local/repo\"\nexport RESTIC_REPOSITORY=\"&lt;YOUR_REPOSITORY_LOCATION&gt;\"\n\n# Repository password\nexport RESTIC_PASSWORD=\"&lt;YOUR_STRONG_PASSWORD&gt;\"\n\n# If using S3:\n# export AWS_ACCESS_KEY_ID=\"&lt;YOUR_ACCESS_KEY&gt;\"\n# export AWS_SECRET_ACCESS_KEY=\"&lt;YOUR_SECRET_KEY&gt;\"\n</code></pre> <p>Secure the environment file: <code>sudo chmod 600 /etc/restic-env</code></p>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#3-creating-the-systemd-service","title":"3. Creating the Systemd Service","text":"<p>Create /etc/systemd/system/restic-backup.service:</p> <pre><code>[Unit]\nDescription=Restic backup service\nAfter=network.target\n\n[Service]\nType=oneshot\nExecStart=/usr/local/bin/backup-script.sh\nUser=root\nGroup=root\n\n# Set reasonable security options\nProtectSystem=full\nProtectHome=read-only\nPrivateTmp=true\nNoNewPrivileges=true\n</code></pre>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#4-setting-up-the-timer","title":"4. Setting Up the Timer","text":"<p>Create <code>/etc/systemd/system/restic-backup.timer</code>:</p> <pre><code>[Unit]\nDescription=Run restic backup every 12 hours\n\n[Timer]\nOnCalendar=_-_-\\* 00,12:00:00\nRandomizedDelaySec=1800\nPersistent=true\n\n[Install]\nWantedBy=timers.target\n</code></pre>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#5-activating-the-backup-system","title":"5. Activating the Backup System","text":"<p>Enable and start the timer:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable restic-backup.timer\nsudo systemctl start restic-backup.timer\n</code></pre>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#backup-schedule-and-retention","title":"Backup Schedule and Retention","text":"<p>This setup:</p> <ul> <li>Creates backups every 12 hours (at midnight and noon)</li> <li>Adds a random delay of up to 30 minutes to avoid exact timing</li> <li>Keeps the last 72 hours of backups (6 snapshots)</li> <li>Maintains one weekly backup for older data</li> <li>Automatically prunes old snapshots</li> </ul>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#useful-commands","title":"Useful Commands","text":"<ul> <li>Check the timer status: <code>sudo systemctl list-timers restic-backup.timer</code></li> <li>Run a backup manually: <code>sudo systemctl start restic-backup.service</code></li> <li>View backup logs: <code>sudo journalctl -u restic-backup.service</code></li> <li>List all snapshots: <code>sudo bash -c 'source /etc/restic-env; restic snapshots'</code></li> <li>Restore files: <code>sudo restic restore latest --target /path/to/restoration/directory</code></li> </ul>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#security-considerations","title":"Security Considerations","text":"<p>The setup includes several security measures:</p> <ul> <li>The environment file is restricted to root access only</li> <li>The systemd service runs with restricted privileges</li> <li>The backup script uses safe bash options</li> <li>File systems are mounted read-only during backups where possible</li> </ul>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#testing-your-backups","title":"Testing Your Backups","text":"<p>Remember the cardinal rule of backups - they're only as good as your ability to restore from them. Regularly test your backup restoration process by:</p> <ul> <li>Creating a test directory</li> <li>Restoring a backup to it</li> <li>Verifying the restored files</li> </ul>","tags":["sysadmin"]},{"location":"blog/2024/11/05/setting-up-automated-backups-with-restic-on-debian-linux/#conclusion","title":"Conclusion","text":"<p>You now have a robust, automated backup system that:</p> <ul> <li>Runs automatically every 12 hours</li> <li>Maintains a sensible backup history</li> <li>Includes important system directories</li> <li>Uses secure settings</li> <li>Can be easily monitored and managed</li> </ul> <p>Remember to periodically verify your backups and adjust the retention policy based on your specific needs. Happy backing up!</p>","tags":["sysadmin"]},{"location":"blog/2024/11/01/librechat-artifacts-content-security-policy/","title":"LibreChat Artifacts Content Security Policy","text":"<p>I remote host LibreChat on a Hetzner VPS and I was trying to use the Artifacts feature but I kept getting an error when loading HTML5 apps in the iframe. In the network tab on my browser's dev tools, I was seeing:</p> <pre><code>Refused to frame 'https://2-18-2-sandpack.codesandbox.io/' because it violates the following Content Security Policy directive: \"default-src 'self'\". Note that 'frame-src' was not explicitly set, so 'default-src' is used as a fallback.\n</code></pre> <p>The fix was adding a new content security policy to my Caddyfile (web server/reverse proxy) allowing the domain sandbox.io to load stuff in the iframe.</p> <pre><code>Content-Security-Policy \"default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; frame-ancestors 'self' your.domain; frame-src 'self' https://*.codesandbox.io\"\n</code></pre> <p>This appears to be a dependency of the sandpack library used by LibreChat to support Artifacts. I sent a PR to help others :thumbsup:</p> <p>My Full Caddyfile config for LibreChat:</p> <pre><code>chat.your.domain {\n    reverse_proxy localhost:3080\n    header {\n        Content-Security-Policy \"default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; frame-ancestors 'self' your.domain; frame-src 'self' https://*.codesandbox.io\"\n\n        X-Frame-Options \"SAMEORIGIN\"\n        X-Content-Type-Options \"nosniff\"\n        Referrer-Policy \"strict-origin-when-cross-origin\"\n        Strict-Transport-Security \"max-age=31536000; includeSubDomains; preload\"\n        Permissions-Policy \"geolocation=(), midi=(), sync-xhr=(), microphone=(), camera=(), magnetometer=(), gyroscope=(), fullscreen=(self), payment=()\"\n    }\n}\n</code></pre>","tags":["librechat","sysadmin"]},{"location":"computers/ai/","title":"AI/ML","text":"<ul> <li>Andrew Ng ML course</li> <li>IBM Data Engineer certification</li> <li>MSFT Gen AI for Beginners</li> <li>Hugging Face learn NLP course</li> <li>AI for Everyone course</li> <li>Practical Deep Learning for Coders and the Jupyter notebook based book</li> <li>Huggingface</li> <li>FastAPI Docker example</li> <li>https://karpathy.ai/zero-to-hero.html</li> </ul>"},{"location":"computers/ai/#general-notes","title":"General Notes","text":"<p>Model is a neural network function.</p> <p>Models alone aren't good enough, they need to be trained and importantly you need the \"weights\" in order to have a ready to use model like the resnet18 model that \"comes with\" FastAI.</p> <p>Fine tuning is taking a trained model and training it further on your data.</p> <p>Learner is model + training data</p> <p>Weights are parameters</p> <ul> <li>It used to be Input &gt; Program &gt; Output</li> <li>Now its Input + Weights &gt; Model &gt; Output</li> <li>With a training loop is Input + Weights + \"Loss\" from previous training &gt; Model &gt; Output + Loss (now do it all again N times to solve any computable function!)</li> </ul>"},{"location":"computers/ai/#embeddings","title":"Embeddings","text":"<p>Credit and sources:</p> <ul> <li>Simon Willison's embeddings article</li> </ul> <p>Using an embeddings model, take content and turn it into a multidimensional array of floating point numbers. The length of the array is always the same regardless of the size of the content; the size is determined by the model used to create the embeddings.</p> <p>Combine embeddings, or rather collections of, with a database and now you basically have semantic search.</p> <p>To use Retrieval Augmented Generation (RAG), you take an LLM plus your collections of embeddings and combined. When a request comes in, first \"semantic search\" for related content within the embeddings then you shove that content (paragraph, sentences, whatever) into the prompt so the LLM will use that when generating the response. Wow genius...</p>"},{"location":"computers/aws/","title":"AWS","text":"<p>Establishing your best practice AWS environment - the short and sweet bible on AWS account organization</p> <p>Multi-region blog series - foundation with AWS security, networking, and compute services - data and replication strategies - application and management layers</p> <p>DR architecture on AWS series</p>"},{"location":"computers/aws/#cognito","title":"Cognito","text":"<p>The two main components of Amazon Cognito are user pools and identity pools. User pools are user directories that provide sign-up and sign-in options for your app users. Identity pools enable you to grant your users access to other AWS services. You can use identity pools and user pools separately or together.</p>"},{"location":"computers/aws/#lambda","title":"Lambda","text":"<p>Default is synchronous, change invocation type to <code>Event</code> to switch to async</p> <ul> <li>when using async, failed requests go to the DLQ</li> <li>change invocation type to <code>DryRun</code> to do just that</li> </ul>"},{"location":"computers/devex/","title":"Developer experience","text":"<p>idk this will be something someday</p>"},{"location":"computers/devex/#rest","title":"REST","text":"<ul> <li>Tablestakes way for users to consume your product programmatically or provide users opportunities to extend/enhance your product</li> <li>Customers understand this, REST has been around forever and people get it</li> </ul>"},{"location":"computers/devex/#graphql","title":"GraphQL","text":"<ul> <li>It's a hurdle for customers still in 2024; might change over time</li> </ul>"},{"location":"computers/devex/#webhooks","title":"Webhooks","text":"<ul> <li>Somewhat the standard along with REST</li> <li>Let's customers \"listen\" for certain events in your product   todo summarize https://github.com/standard-webhooks/standard-webhooks/blob/main/spec/standard-webhooks.md</li> </ul> <p>Start thin, get fuller as necessary. 20kb max size.</p> <p>HMAC for signing is pretty standard. This plus HTTPS to guarantee sender + integrity of payload. Treat pre shared keys like secrets duh.</p>"},{"location":"computers/devex/#cli","title":"CLI","text":"<ul> <li>Devs, DevOps, SRE, platform, systems people all love CLIs</li> <li>You can drop them into CI systems and other workflows, they are portable</li> </ul>"},{"location":"computers/devex/#kubectl","title":"kubectl","text":"<ul> <li>Cam across this recently with CoreWeave, they use kubectl as a client which is pretty rad</li> <li>Obviously a super narrow usecase but inspirational</li> </ul>"},{"location":"computers/enablement/","title":"Engineering enablement","text":""},{"location":"computers/enablement/#metrics","title":"Metrics","text":"<p>Wisdom from Abi Noda</p> <p>Calculating engineering metrics perfectly is impossible. Take cycle time, for example:</p> <ul> <li> <p>Calculating it from PR open to merge creates fuzziness based on when devs open PRs</p> </li> <li> <p>If you try to calculate from \"first commit\", Git rebases muddy your numbers</p> </li> <li> <p>Also: your metrics aren't factoring out non-working hours or weekends</p> </li> <li> <p>And some teams don't use PRs at all</p> </li> </ul> <p>It's fine to settle for \"good enough\".</p> <p>All metrics are proxies.</p> <p>It's the trends and comparisons that give you the most signal.</p>"},{"location":"computers/go/","title":"Go cookbook","text":""},{"location":"computers/go/#web-requests","title":"Web requests","text":"<p>Boilerplate and timesavers.</p> <pre><code>package main\n\nimport (\n    \"net/http\"\n    \"time\"\n)\n\ntype Client struct {\n    httpClient http.Client\n}\n\n// build a re-usable client with some config e.g. timeout\nfunc NewClient() Client {\n    return Client{\n        httpClient: http.Client{\n            Timeout: time.Minute,\n        },\n    }\n}\n\n</code></pre>"},{"location":"computers/go/#structs","title":"Structs","text":""},{"location":"computers/go/#nested-structs","title":"Nested structs","text":"<p>Structs can be nested to represent more complex entities.</p> <pre><code>type car struct {\n  Make string\n  Model string\n  Height int\n  Width int\n  FrontWheel Wheel\n  BackWheel Wheel\n}\n\ntype Wheel struct {\n  Radius int\n  Material string\n}\n</code></pre> <p>The fields of a struct can be accessed using the dot . operator.</p> <pre><code>myCar := car{}\nmyCar.FrontWheel.Radius = 5\n</code></pre>"},{"location":"computers/go/#embedded-structs","title":"Embedded structs","text":"<pre><code>type car struct {\n  make string\n  model string\n}\n\ntype truck struct {\n  // \"car\" is embedded, so the definition of a\n  // \"truck\" now also additionally contains all\n  // of the fields of the car struct\n  car\n  bedSize int\n}\n</code></pre> <p>An embedded struct's fields are accessed at the top level, unlike nested structs. Promoted fields can be accessed like normal fields except that they can't be used in composite literals.</p> <pre><code>lanesTruck := truck{\n  bedSize: 10,\n  car: car{\n    make: \"toyota\",\n    model: \"camry\",\n  },\n}\n\nfmt.Println(lanesTruck.bedSize)\n\n// embedded fields promoted to the top-level\n// instead of lanesTruck.car.make\nfmt.Println(lanesTruck.make)\nfmt.Println(lanesTruck.model)\n</code></pre>"},{"location":"computers/go/#working-with-json","title":"Working with JSON","text":"<p>The <code>WebhookPayload</code> structure in the Go code represents the JSON payload that your webhook expects to receive. Let's break down the nested structure to understand how it correlates with your JSON format:</p> <ol> <li> <p>Overall Structure: <code>WebhookPayload</code> is designed to match the top-level structure of your JSON payload. It contains two fields, <code>Component</code> and <code>Parameters</code>, each corresponding to a key in your JSON.</p> </li> <li> <p>The <code>Component</code> Field:</p> </li> <li> <p>This is a nested structure within <code>WebhookPayload</code>.</p> </li> <li>It corresponds to the <code>\"component\"</code> key in your JSON.</li> <li>It is defined as a struct <code>Component</code>, which has fields <code>ID</code>, <code>Name</code>, and <code>Repository</code>.</li> <li> <p>These fields are meant to directly map to the nested JSON object under the <code>\"component\"</code> key.</p> </li> <li> <p>The <code>Parameters</code> Field:</p> </li> <li> <p>This represents the <code>\"parameters\"</code> key in your JSON.</p> </li> <li>It's defined as a <code>map[string]string</code>.</li> <li>This is a flexible structure where keys and values are both strings.</li> <li> <p>It's used here because your parameters appear to be a collection of key-value pairs where both the keys and the values are strings.</p> </li> <li> <p>JSON Tags:</p> </li> <li>Notice the <code>json:\"...\"</code> tags in the struct definitions. These tags tell the Go <code>json</code> package how to map the JSON keys to struct fields.</li> <li>For example, <code>ID string json:\"id\"</code> in the <code>Component</code> struct means that the <code>ID</code> field in Go is mapped to the <code>\"id\"</code> key in your JSON payload.</li> </ol> <p>Here's a visual breakdown:</p> <p>Your JSON Payload:</p> <pre><code>{\n  \"component\": {\n    \"id\": \"...\",\n    \"name\": \"...\",\n    \"repository\": \"...\"\n  },\n  \"parameters\": {\n    \"key1\": \"value1\",\n    \"key2\": \"value2\"\n    // ... more key-value pairs ...\n  }\n}\n</code></pre> <p>Mapped to Go Structs:</p> <pre><code>type WebhookPayload struct {\n    Component  Component  `json:\"component\"`  // Maps to \"component\" in JSON\n    Parameters Parameters `json:\"parameters\"` // Maps to \"parameters\" in JSON\n}\n\ntype Component struct {\n    ID         string `json:\"id\"`          // Maps to \"id\" in component\n    Name       string `json:\"name\"`        // Maps to \"name\" in component\n    Repository string `json:\"repository\"`  // Maps to \"repository\" in component\n}\n\ntype Parameters map[string]string // Maps to key-value pairs in \"parameters\"\n</code></pre> <p>When a JSON payload is received, Go's <code>json.Unmarshal</code> function uses these definitions to correctly parse the JSON into a <code>WebhookPayload</code> object, making the data structured and easy to work with in your code.</p>"},{"location":"computers/go/#example","title":"Example","text":"<p>Handling a webhook of a specific shape. e.g.</p> <pre><code>{\n  \"component\": {\n    \"id\": \"...\",\n    \"name\": \"...\",\n    \"repository\": \"...\"\n  },\n  \"parameters\": {\n    \"key1\": \"value1\",\n    \"key2\": \"value2\"\n    // ... more key-value pairs ...\n  }\n}\n</code></pre> <pre><code>package main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"log\"\n    \"net/http\"\n)\n\n// Define structs to match the JSON structure\ntype Component struct {\n    ID         string `json:\"id\"`\n    Name       string `json:\"name\"`\n    Repository string `json:\"repository\"`\n}\n\ntype Parameters map[string]string\n\ntype WebhookPayload struct {\n    Component  Component  `json:\"component\"`\n    Parameters Parameters `json:\"parameters\"`\n}\n\nfunc webhookHandler(w http.ResponseWriter, r *http.Request) {\n    if r.Method != \"POST\" {\n        http.Error(w, \"Only POST method is accepted\", http.StatusMethodNotAllowed)\n        return\n    }\n\n    body, err := ioutil.ReadAll(r.Body)\n    if err != nil {\n        http.Error(w, \"Error reading request body\", http.StatusInternalServerError)\n        return\n    }\n    defer r.Body.Close()\n\n    var payload WebhookPayload\n    if err := json.Unmarshal(body, &amp;payload); err != nil {\n        http.Error(w, \"Error decoding JSON\", http.StatusBadRequest)\n        return\n    }\n\n    // Process the payload here\n    fmt.Printf(\"Received component: %+v\\n\", payload.Component)\n    fmt.Printf(\"Received parameters: %+v\\n\", payload.Parameters)\n\n    // Respond to the webhook sender\n    fmt.Fprintf(w, \"Webhook received and processed\")\n}\n\nfunc main() {\n    http.HandleFunc(\"/webhook\", webhookHandler)\n    log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n\n\n</code></pre>"},{"location":"computers/go/#send-tokens-to-a-channel","title":"Send tokens to a channel","text":"<p>Waiting for databases to come online for example.</p> <pre><code>func getDatabasesChannel(numDBs int) chan struct{} {\n    ch := make(chan struct{})\n    go func() {\n        for i := 0; i &lt; numDBs; i++ {\n            ch &lt;- struct{}{} // send an empty struct, used as a \"token\"\n            fmt.Printf(\"Database %v is online\\n\", i+1)\n        }\n    }()\n    return ch\n}\n</code></pre> <p>Send an empty struct, <code>struct{}{}</code>, to a channel to be used as a \"token\".</p> <p>Then block and wait for a specific number of tokens in another function.</p> <pre><code>    for i := 0; i &lt; numDBs; i++ {\n        &lt;-dbChan // unary operator pops off empty struct token from queue\n    }\n}\n</code></pre>"},{"location":"computers/kids/","title":"Kids","text":"<p>First computer: iPad Air 2, iPad Air Pro 11in First video game console: Nintendo Switch first gen</p>"},{"location":"computers/platforms/","title":"On platforms","text":"<p>As a product manager working on software products for a tech company, platforms are something I think about a lot.</p>"},{"location":"computers/platforms/#links","title":"Links","text":"<ul> <li>https://learn.microsoft.com/en-gb/platform-engineering/about/principles</li> </ul>"},{"location":"computers/platforms/#mind-the-platform-execution-gap","title":"Mind the platform execution gap","text":"<p>Notes from this Martin Fowler article</p> <p>A platform is a foundation of self-service APIs, tools, services, knowledge and support which are arranged as a compelling (internal) product.</p> <p>The purpose of a developer productivity platform is to allow teams who build end-user products concentrate on their core mission.</p> <p>An internal platform team will usually take tools and services offered by cloud providers and other vendors and host, adapt or extend them to make them conveniently available to their software developer colleagues. The aim is not to reinvent commercially available functionality (the world does not need another homegrown Kubernetes) but to bridge the gap between what you can buy and what is really needed (your teams may appreciate a simplified Kubernetes experience that takes advantage of assumptions about your infrastructure and makes it easier to manage).</p> <p>A what-it-is-for rather than a how-it-is-made view of platform is preferable because offering platform services to internal teams is an institutionalised approach to reducing friction. It is incumbent upon platform engineers to keep an open mind about the best way to reduce that friction. Some days that will be provisioning infrastructure. Other days it might be making a build script a little easier to use or facilitating a workshop to help a team to define their SLOs.</p> <p>When it goes wrong, problems with the platform are passed directly onto the entire software development organisation.</p> <p>Your users appreciate consistency, stability and dependability over a stream of new features. </p> <p>Deprecation is a fundamental part of the platform product lifecycle, and failure to consider it may undermine the business benefits you hoped to gain by offering it in the first place.</p> <p>Anything that prevents developers from smoothly using your platform, whether a flaw in API usability or a gap in documentation, is a threat to the successful realisation of the business value of the platform. Prioritise developer experience.</p> <p>Platform products require customer empathy, product ownership and intelligent measurement, just like other kinds of product.</p> <p>[Users] need to appreciate it, understand it and be aware of its features.</p> <p>You have access to lots of feedback/communication internally, use it but also be ready for a lot of blunt/candid feedback. Harness your champions, encourage them to evangelize and always have a group of champions on your side you meet with regularly. You still need a roadmap and need to communicate it. Don't forget to market internally as well, just as important for awareness.</p> <p>Maintaining goodwill and trust is key to adoption and continued usage.</p>"},{"location":"computers/pm/","title":"Product Management","text":"<p>Make it work, then make it right, then make it fast. The engineers you work with will thank you.</p>"},{"location":"computers/pm/#launching-a-new-saas-product","title":"Launching a new SaaS product","text":"<p>Gatekeep your alpha/beta, do not go \"open beta\" until you are forced to. Talk to all of your \"closed beta\" customers regularly, know them by name and keep refining. Let more customers in if you aren't getting the feedback you need OR if things are booming and you know you are on to something. But be judicious if you expand the cohort, you must keep talking to whoever you let in.</p> <p>Always be talkin to customers, duh.</p> <p>Instrument the shit out of your product with an analytics tool. Currently a huge fan of Amplitude and Intercom.</p> <p>Find PMF by asking users How upset would you be if the product went away?</p> <ul> <li>Very sad</li> <li>Sad</li> <li>Not sad at all</li> </ul> <p>Then measure</p> <ul> <li>MAU / MAI, (Monthly actives over monthly instances aka tenants) - How many users per tenant shows adoption</li> <li>Pay attention to Churn at W2, W3, W4</li> <li>Have a funnel, know your funnel, analyze your funnel</li> </ul> <p>Refine, refactor, invest deeper into every experience you build going forward. Likely taking on a ton of work at this point and letting things bake and getting customer guidance is now critical.</p> <p>Have a 3-6 months roadmap and keep focusing, expand when you have clear signal for what's next AND your funnel is healthy at the top.</p> <p>Start focusing on completing journeys/jobs-to-be-done. Make sure the product tells a story. One really solid usecase with a clear \"we do X better than A, B, C\" story is worth WAY more than 10 half assed usecases. Caveat here is you can get away with half assed (at least for a few years) if your new product is being built inside an already very successful/large company but you will still enjoy much better success if you focus :) (speaking from experience building a new SaaS product for developers at Atlassian)</p>"},{"location":"computers/pm/#developer-experience-extensibility","title":"Developer Experience / Extensibility","text":"<p>Think about extensibility and developer experience with your product from day 1. If you're building something SaaS chances are you'll want this at some point (e.g. integrations); doubly so if you're a product sold to devs/eng. APIs, SDKs, CLI tools, etc. get it out early and refine it openly.</p> <p>Carve out a DevEx for the path YOU WANT 3P developers to build towards. Figure out what that core experience is, the best way they can extend your platform, and hand hold them down it. Build your base.</p>"},{"location":"computers/pm/#on-integrations-with-dev-tools","title":"On integrations with dev tools","text":"<p>Trying to build lots of plugins/integrations for 3P developer systems/tools is hard and doesn't scale well. You will ship a lot of v1's and never come back to them if you try a \"get names on the box\" to attract customers approach. Invest in the most critical apps that will actually get used; build magic experiences for top requested integrations worth the most deals/seats. If you spend 2 eng time for 2 sprints on an app nobody uses for 12+ months just to potentially attract customers was it worth it? Name on the box helps much less compared to a strong \"we do X,Y,Z better than all our competitors\" pitch using just a few (or one!) really strong usecase.</p>"},{"location":"computers/pm/#things-to-emulate-or-learn-from","title":"Things to emulate or learn from","text":""},{"location":"computers/pm/#articles-and-blog-posts","title":"Articles and blog posts","text":"<p>Building and running modern software is hard.</p> <ul> <li>DevOps is Bullshit</li> <li>Who should write the Terraform?</li> <li>Mind the platform execution gap</li> <li>Gardening platforms - the format is really obnoxious but there is some decent nuggets in there</li> <li>https://blog.jim-nielsen.com/2022/website-fidelity/</li> <li>Video on moving fast from YouTube</li> </ul>"},{"location":"computers/pm/#sdk-api-dev-docs","title":"SDK, API, Dev Docs","text":"<p>Good examples to emulate and learn from:</p> <ul> <li>https://news.ycombinator.com/item?id=32794330</li> <li>https://tailwindcss.com/docs/installation</li> <li>https://www.solidjs.com/tutorial/introduction_signals</li> <li>https://www.postgresql.org/docs/current/index.html</li> <li>https://docs.djangoproject.com/en/4.1/</li> <li>https://learn.microsoft.com/en-us/dotnet/api/system.linq.enumerable.distinct?view=netcore-3.0</li> <li>https://redis.io/commands/</li> </ul>"},{"location":"computers/service-mesh/","title":"Service mesh use cases","text":"<p>Mirror of Service mesh use cases - lucperkins.dev</p>"},{"location":"computers/service-mesh/#service-discovery","title":"Service discovery","text":"<p>TL;DR: Access other services in the network using simple names.</p> <p>Your services need to be able to automatically \u201cfind\u201d each other via readily comprehensible names, such as service.api.production or pets/staging or cassandra. Cloud environments are highly elastic environments where many instances of a service will be gathered behind that name and hard-coding IP addresses is thus a complete non-starter.</p> <p>And once a service finds another service, it needs to be able to send requests to that service without worrying that it\u2019s going to hit a non-responsive service instance. That means that the service mesh needs to keep track of instance liveness and keep the host list as up to date as possible.</p> <p>Different service meshes provide service discovery in different ways. Right now it seems most common to delegate service discovery to an external mechanism like Kubernetes DNS. At Twitter back in the day we used the Finagle naming system. Service mesh even paves the way to fully custom naming mechanisms, though I haven\u2019t seen one that provides this.</p>"},{"location":"computers/service-mesh/#encryption","title":"Encryption","text":"<p>TL;DR: Eliminate unencrypted traffic between services in a scalable, automated way.</p> <p>It\u2019s nice to ensure that malicious agents can\u2019t penetrate into your internal network. Firewalls are good for that. But what happens if they manage to break in? Should they be able to run amok, snooping inter-service traffic at will? Let\u2019s hope not. In order to prevent this scenario, you need to establish a zero-trust network in which all traffic between all of your services is encrypted. Most existing service meshes do this by providing mutual TLS (mTLS). And in some cases, mTLS works across clouds and clusters and someday probably planets.</p> <p>You don\u2019t need a service mesh for mTLS, of course. You could let each service handle its own TLS, but that would mean that you need to figure out a way to generate certs, distribute them to your service hosts, write application code that pulls in those certs from the filesystem, and then rotate those certs at established intervals. Service meshes automate mTLS using systems like SPIFFE that in turn automate certificate issuance and rotation.</p>"},{"location":"computers/service-mesh/#authentication-and-authorization","title":"Authentication and authorization","text":"<p>TL;DR: Determine who is making a request and determine what they\u2019re allowed to do before the request even reaches your service.</p> <p>Services often need to know who is making a request (authentication) and, on the basis of that ascertained identity, decide what they\u2019re allowed to do (authorization). The \u201cwho\u201d here can be either:</p> <p>Other services. This is called peer authentication. The web service wants to access the db service. Service meshes typically solve this via mTLS, whereby certificates provide the necessary ID card.</p> <p>Specific human users. This is called request authentication. User haxor69 wants to purchase a new lamp. Service meshes provide various mechanisms for this, such as JSON Web Tokens.</p> <p>This is something that we\u2019re all used to doing in our application code. A request comes in, we ask our users table if the user exists and has provided the right password, then we check the permissions column in our users table, etc. With service mesh this transpires before the request even reaches your service.</p> <p>Once you figure out who a request is coming from, you need to determine what they\u2019re authorized to do. Some service meshes enable you to define simple policies about who gets to do what using YAML or the command line, while others offer integrations with generalized frameworks like Open Policy Agent. The end goal is that your services can safely assume that every request is from an allowed party and that the action is authorized.</p>"},{"location":"computers/service-mesh/#load-balancing","title":"Load balancing","text":"<p>TL;DR: Distribute load across service instances in accordance with a desired pattern.</p> <p>A \u201cservice\u201d in a service mesh is very often comprised of multiple identical service instances, such as a cache service that has 5 instances today but might have 11 tomorrow. When requests are made to the cache service, those need to be distributed in accordance with some goal, such as minimizing response latency or maximizing the likelihood of reaching a healthy instance on the first try. Round-robin load balancing tends to be the most common pattern, but there are many others, such as weighted request (you can select targets to \u201cfavor\u201d), ring hash (using consistent hashing for upstream hosts), or least requests (the instance receiving the least requests is favored).</p> <p>Traditional balancers offer other features like HTTP caching and DDoS protection, but those are less widely used for east-west traffic (the standard domain for service mesh). You don\u2019t need to use a service mesh for load balancing, of course, but service meshes do enable you to control load balancing policies on a per-service basis and from a centralized control plane, eliminating the need to run and configure dedicated load balancers in your networking stack.</p>"},{"location":"computers/service-mesh/#circuit-breaking","title":"Circuit breaking","text":"<p>TL;DR: Stop traffic to a struggling service and control damage from worst-case scenarios.</p> <p>If a service is failing to keep up with traffic for whatever reason, service meshes provide you a range of options for coping with this (a few are covered in other sections). Circuit breaking is the \u201cnuclear option\u201d of stopping traffic to the service. But don\u2019t want to just stop traffic cold; you need to specify a fallback plan. That could mean applying backpressure into requesting services (make sure to configure your mesh for that!) or it could mean something like updating your status page to red and directing users to a \u201cfail whale\u201d page.</p> <p>Service meshes enable you not just to specify both when circuit breaking should occur and what happens when it does. The \u201cwhen\u201d could mean that any number of specified maxima are exceeded: total requests in the last time period, concurrent connections, pending requests, active retries, etc.</p> <p>Circuit breaking isn\u2019t something you want to do often, but it is good to be able to have an absolute last-ditch fallback plan in place.</p>"},{"location":"computers/service-mesh/#autoscaling","title":"Autoscaling","text":"<p>TL;DR: Add or remove service instances automatically based on specified criteria.</p> <p>Service meshes aren\u2019t schedulers, so they don\u2019t perform autoscaling themselves. But they can provide the information that schedulers can use to make those decisions. Because service meshes have access to all traffic between services, they\u2019re privy to a wealth of information about what\u2019s going on\u2014which services are struggling, which are so lightly trafficked that capacity is being wasted, etc.</p> <p>Kubernetes, for example, can autoscale services based on CPU and memory usage of Pods, but if you want to scale based on anything else, in our case anything traffic related, you need a custom metric. A tutorial like this one shows you how to do this with Envoy, Istio, and Prometheus but it\u2019s pretty complex. I\u2019d love to see a scheduler/service mesh make this simpler, enabling you to specify something like \u201cscale the auth service up if it\u2019s exceeded a pending requests threshold for more than a minute.\u201d</p>"},{"location":"computers/service-mesh/#canary-deployments","title":"Canary deployments","text":"<p>TL;DR: Roll out features or versions of a service only to a subset of users.</p> <p>Let\u2019s say you\u2019re building a SaaS product and you have an exciting new version of your service that you want to roll out. You\u2019ve tested it in staging and everything seems fine. But you\u2019re not sure how it will do in the wild and don\u2019t want to burn your credibility with users. Canary deployments are the art of exposing that feature only to some subset of users. Perhaps you want it rolled out to your most loyal users, users on the free tier, or users that have specifically opted to be guinea pigs.</p> <p>Service meshes enable you to do this by specifying criteria that determine who sees which version and routing traffic appropriately. The services themselves don\u2019t need to make that decision. Version 1 of the service can assume that all requests are from users who are supposed to see that version, and version 1.1 can assume the same thing. Meanwhile, you can slowly shift a higher and higher percentage of traffic to the new version if things are going smoothly and your guinea pigs are giving the thumbs up.</p>"},{"location":"computers/service-mesh/#blue-green-deployments","title":"Blue-green deployments","text":"<p>TL;DR: Roll out that hot new feature, but be prepared to instantly roll back.</p> <p>Blue-green deployments involve releasing a new service by running your new \u201cblue\u201d service alongside your current \u201cgreen\u201d service. If things go smoothly and the new service is battle tested you can decommission the green service. And someday, what once was blue will sadly turn green and fade away. Blue-green deployments are unlike canary deployments because they\u2019re designed to go out to all users; they just provide a safe harbor if something goes wrong.</p> <p>Service meshes provide a highly convenient lever for pulling the plug on the blue service and instantly switching back to the safer green service. Not to mention that they provide a lot of insight that you may need in determining whether or not the blue service is ready to go.</p>"},{"location":"computers/service-mesh/#health-checking","title":"Health checking","text":"<p>TL;DR: Determine which service instances are in ship shape and respond to cases when they\u2019re not.</p> <p>Health checking is the act of making judgments about whether service instances are currently read to handle traffic. For HTTP services, for example, that might mean that you make a GET request to a /health endpoint and 200 OK means healthy and anything else means unhealthy. Service meshes enable you to specify both how health is determined and how frequently health is checked. That information can then be used for the purpose of other things, like load balancing and circuit breaking.</p> <p>So health checking isn\u2019t usually a \u201cuse case\u201d in itself insofar and is typically used to further other goals. But you also may need to act on the results of health checks in ways that are extrinsic to other service mesh goals (e.g. updating a status page, creating a GitHub issue, or filing a JIRA ticket). And the service mesh provides a convenient mechanism for automating that.</p>"},{"location":"computers/service-mesh/#load-shedding","title":"Load shedding","text":"<p>TL;DR: Re-route traffic in response to a temporary spike in usage.</p> <p>If you have services that are getting clobbered with traffic, you may want to temporarily redirect, i.e. shed, some of that traffic elsewhere, potentially to a backup service or datacenter or to a persistent Pulsar topic. This enables the current service to continue to service some requests instead of falling over. Load shedding is better than circuit breaking, so you still don\u2019t want to do it often. But it may prevent cascading failures that cripple downstream services.</p>"},{"location":"computers/service-mesh/#traffic-shadowing","title":"Traffic shadowing","text":"<p>TL;DR: Send a single request to multiple places.</p> <p>Sometimes you want a given request, or some sampling of requests, to be \u201cshadowed\u201d to multiple services. One common example is routing a subset of production traffic to a staging service. Your main production web server makes a request to the downstream products.production service and only that service. But the service mesh intelligently copies that request and sends it to products.staging without the web server even knowing.</p> <p>A subordinate use case of service mesh that could be built on top of traffic shadowing is regression testing, which is the practice of giving different versions of the same service the same requests and checking whether both services exhibit the same behavior. I haven\u2019t yet seen a service mesh that directly integrates with a regression testing system like Diffy but it strikes me as a fruitful possibility.</p>"},{"location":"computers/service-mesh/#isolation","title":"Isolation","text":"<p>TL;DR: Divide your mesh into mini-meshes.</p> <p>Also known as segmentation, isolation is the art of dividing your service mesh into logically separate sub-networks that have no knowledge of one another. Isolation is a bit like creating virtual private networks, but with the crucial difference that you still get all the service mesh goodies, like service discovery, but with added security. If a malicious agent somehow got into a service in one sub-network, they wouldn\u2019t be able to, for example, see which services are running in other sub-networks or snoop traffic therein.</p> <p>The benefits may be organizational as well. You may want to divide services into sub-networks based on division of the company and unburden developers of the cognitive load of reasoning about the entire service mesh.</p>"},{"location":"computers/service-mesh/#retries-rate-limiting-and-timeouts","title":"Retries, rate limiting, and timeouts","text":"<p>TL;DR: Strike bread-and-butter request management tasks from your codebase.</p> <p>These could all be thought of as separate use cases but I\u2019ll smoosh them together here because what they share in common is that they\u2019re request lifecycle tasks typically handled by application libraries. If you\u2019re writing a (non-service-meshed) Ruby on Rails web server that makes requests to backend services via gRPC, your application code will need to determine what happens when N requests fail, you\u2019ll need to figure out how much traffic those services can handle and hardcode that via a rate limiting library, and you\u2019ll need to decide when to give up and let the request timeout. And if you want to update any of the above, you\u2019ll need to stop, reconfigure, and restart.</p> <p>Delegating this to the service mesh means not only that service engineers don\u2019t need to think about them but also that they can be reasoned about in a more global way. If you\u2019re working with a complex chain of services, e.g. A \u2013&gt; B \u2013&gt; C \u2013&gt; D \u2013&gt; E, you\u2019ll want to reason about the entire request lifecycle. If you need to extend timeouts in service C, you want to pull an Archimedean lever to do so, not update service code and wait for the pull request to be approved and CI to redeploy the service.</p>"},{"location":"computers/service-mesh/#telemetry","title":"Telemetry","text":"<p>TL;DR: Gather all the information as you need from your services, plus some more just in case.</p> <p>Telemetry is a catch-all term for metrics, distributed tracing, and logs. Service meshes provide mechanisms for handling and acting upon all three. This is where things get a bit soupy because there are so many options. For metrics you have Prometheus et al, for logs you have fluentd, Loki, and Vector et al, for distributed tracing you have Jaeger et al. Specific service meshes tend to have built-in integrations for some systems and not others. It remains to be seen if the Open Telemetry project provides some convergence.</p> <p>The benefit of service mesh here is that sidecars can, in principle, collect all of the above from the services running next to them. This means that you have a single telemetry gathering system at your disposal, which enables the service mesh to handle that information in any number of ways. To give a few examples:</p> <p>Tail the logs from a specific service using the CLI Monitor request volume from the service mesh dashboard Gather distributed tracing data and feed that into a system like Jaeger Value judgment alert: In general, telemetry is an area where you probably don\u2019t want your service mesh to do too much. Getting some really basic insight and checking on \u201cgolden metrics\u201d like success rates and latency on the fly is fine, but let\u2019s hope that we don\u2019t see the rise of service mesh Frankenstacks that try to displace other single-purpose systems, some of which are quite good and well understood.</p>"},{"location":"computers/service-mesh/#auditing","title":"Auditing","text":"<p>TL;DR: Those who don\u2019t know history are condemned to repeat it.</p> <p>Auditing is the art of keeping track of important events in a system. In a service mesh, that might mean knowing who has made requests to specific endpoints of specific services or how many times a particular security-related event has transpired in the last month.</p> <p>Auditing is very closely related to telemetry, of course, but with the difference that telemetry is typically associated with things like performance and technical soundness, whereas auditing may have legal and other ramifications that extend outside of the strictly technical sphere (GDPR compliance, anyone?).</p>"},{"location":"computers/service-mesh/#visualization","title":"Visualization","text":"<p>TL;DR: All hail the fancy React.js dashboard!</p> <p>There may be a more specific term for this but I\u2019m not aware of it. I just mean creating a visual representation of your service mesh or some subset thereof, and these visualizations can include things like average latency metrics, sidecar configuration information, health check results, and alerts.</p> <p>Working in a service-oriented environment already bears significant cognitive load vis-\u00e0-vis working on a Majestic Monolith. Anything you can do to reduce cognitive costs is a win, and something as simple as letting developers click around on a visual representation of the mesh may provide crucial grounding (and is great for onboarding!).</p>"},{"location":"computers/terraform/","title":"Terraform best practices","text":"<p>Lock versions! Modules, providers, and Terraform itself. The devil you know is better than the devil you don't. Use either Docker <code>docker run -v $(pwd):/&lt;dir name&gt;/ -w /&lt;dir name&gt;/ --rm -it hashicorp/terraform:light &lt;terraform command&gt;</code> or <code>tfenv</code>. Use a shell alias or bash wrapper to make life easier for the version you standardize on!</p> <p>Don't forget about .gitignore</p> <pre><code>terraform.tfstate.backup\nterraform.tfvars\n.terraform/\n</code></pre> <p>Organize based on what is within your control. You cannot control reorgs, for example.</p> <p>Tag every resource liberally. Owner/contact, service role, environment, biz unit, managed_by=Terraform, etc. No tags, no merge!</p> <p>Use remote state. Seems obvious, basically a requirement when operating as a team. Secured and backed up! Use state lock to prevent collisions. Don't commit state to git...</p> <ul> <li>Use a <code>data source</code> with TF remote state to share things like IDs across state files. Example:</li> </ul> <pre><code># stateful service outputs a SQL DB id\noutput \"sqldb_id\" {\n  value       = azurerm_sql_database.example.id\n  description = \"Database ID\"\n}\n\n# stateless service references that ID for configuration\ndata \"terraform_remote_state\" \"dev_sqldb\" {\n  backend = \"azurerm\"\n\n  config = {\n    storage_account_name = \"terraformsa\"\n    container_name       = \"terraformstate\"\n    key                  = \"development/sqldb.tfstate\"\n  }\n}\n\n# now using in a file\ndata.terraform_remote_state.dev_sqldb.outputs.sqldb_id\n</code></pre> <p>Look at using community modules instead of rolling your own first. Don't reinvent the wheel, look for the most downloaded/used modules. That said, you need to know what is getting created and it shouldn't be a mystery when using community modules. If filling in 50+ parameters (e.g. EKS module), including for features you may not use, it's ok to take a step back and consider building your own module to make it more maintainable and understandable.</p> <p>When you do create your own modules, strive to keep them clean and stateless.</p> <p>Apply coding best practices. KISS, DRY, linting, formatting, pull requests with reviewers before merging and CI.</p> <p>Do:</p> <ul> <li>Use git for managing TF files</li> <li>KISS &amp; DRY yet Human &amp; Clean (humans read and maintain Terraform)</li> <li>Functional programming &amp; idempotency</li> </ul>"},{"location":"computers/terraform/#variables-and-locals","title":"Variables and locals","text":"<p>Variables, their purpose is for settings for module configuration. Make sure to set a default or validation! If used just once, set a default. If used per region/etc, use tfvars file instead.</p> <pre><code>variable \"az_names\" {\n    type = list(string)\n    default = [\"use-west-2a\"]\n}\n\ntfvars:\naws_account = \"xxx\"\naws_region = \"us-west-2\"\ndc = \"foo\"\nzones = [\"a\"]\nsubnet = \"bar\"\n</code></pre> <p>Locals can use expressions and resource arguments so they are dynamic. Used in modules. Do things like enforcement of tag names or bucket names. Consider the local a constant to be relied on. Be aware of cloud provider character limits.</p> <pre><code>locals{\n    bucket_name = \"${local.company_name}-protected-${local.suffix}-${var.dc}\"\n}\n\n</code></pre> <p>Do:</p> <ul> <li>keep all variables in one file</li> <li>use tfvars where necessary</li> <li>utilize env vars</li> <li>use for locals de-hardcoding one-time names, DRY</li> <li>keep things generic</li> <li>leave logic to modules, KISS</li> <li>avoid using locals outside of modules</li> <li>pass outputs to modules using `data`` sources</li> </ul> <p>Don't:</p> <ul> <li>use multiple locals blocks if not totally necessary, hard to track and maintain</li> <li>decentralize vars/tfvars, keep them in one file for easier maintenance</li> <li>ignore env vars</li> </ul> <p>Ugly:</p> <ul> <li>Hardcoded variable values where not necessary</li> </ul>"},{"location":"computers/terraform/#execution","title":"Execution","text":"<p>Do:</p> <ul> <li>use remote execution</li> <li>use TF apply with a plan file</li> <li>setup a TF timeout so you aren't waiting forever when something doesn't go right</li> </ul> <p>Don't:</p> <ul> <li>execute locally, keep state where it's supposed to be</li> </ul> <p>Ugly:</p> <ul> <li>execute locally and Ctrl+C to fubar your state</li> </ul>"},{"location":"computers/terraform/#practices-enforcement","title":"Practices enforcement","text":"<ul> <li>Tag resources</li> <li>Linting (<code>tflint</code>) &amp; formatting (<code>terraform fmt</code>)</li> <li>Clean and reusable</li> <li>Documentation</li> <li>More complex things like notify a FinOps team if expected spend is over a threshold</li> </ul> <p>Do:</p> <ul> <li>pre-merge linters, formatters, and logical checks</li> <li>GitHub actions/CI pipeline checks</li> <li>Slack bot drift reporter</li> <li>main branch = actual environment</li> </ul> <p>Security issues and non-compliant configs if you don't have any practices enforcement.</p>"},{"location":"computers/terraform/#structure","title":"Structure","text":"<p>Huge state files have a big blast radius, you want small state files. Best:</p> <pre><code>modules/\n  my-cool-service/\n    global/\n    regional/\nroot-modules/\n  my-cool-service/\n    development/\n      global/\n      us-west-2/\n    staging/\n      global/\n      us-west-2/\n    production/\n      alpha/\n        global/\n        us-east-2/\n        us-west-2/\n      beta/\n        global/\n        us-east-2/\n        us-west-2/\n      gamma/\n        global/\n        us-east-2/\n        us-west-2/\n  an-equally-cool-service/\n    development/\n      global/\n      us-west-2/\n    staging/\n      global/\n      us-west-2/\n    production/\n      global/\n      us-west-2/\n</code></pre> <p>Good:</p> <pre><code>root-modules/\n  my-cool-service/\n    development/\n        us-west-2/\n    staging/\n        us-west-2/\n    production/\n        us-west-2/\n  an-equally-cool-service/\n    development/\n        us-west-2/\n    staging/\n        us-west-2/\n    production/\n        us-west-2/\n</code></pre>"},{"location":"computers/terraform/#workspaces","title":"Workspaces","text":"<p>Use-cases:</p> <ul> <li>different environments (??? double check this, think Hashi says not to use for this)</li> <li>different regions</li> <li>Same config, different customers/tenants</li> <li>test a set of changes before modifying prod</li> </ul> <p>Use a TF wrapper to make sure you're using the right workspace!</p>"},{"location":"computers/terraform/#tools","title":"Tools","text":"<p>Atlantis</p> <ul> <li>Handles TF lock</li> <li>Manages state and history</li> <li>Ensures main branch is the actual environment state.</li> </ul> <p>Others</p> <ul> <li>TFLint</li> <li>TFsec</li> <li>Infracost</li> <li>Inframap</li> <li>ValidIaC</li> <li>Terratest</li> <li>Terratag</li> <li>Terragrunt</li> <li>Checkov</li> <li>KICS</li> <li>Super-Linter</li> </ul>"},{"location":"computers/terraform/#references","title":"References","text":"<ul> <li>https://www.youtube.com/watch?v=U_CsR5ibrOI</li> <li>https://blog.gitguardian.com/infrastructure-as-code-security-best-practices-cheat-sheet-included/</li> <li>https://spacelift.io/blog/terraform-state</li> <li>https://substrate.tools/blog/terraform-best-practices-for-reliability-at-any-scale</li> </ul>"},{"location":"computers/tpm/","title":"Technical Program Management","text":"<p>Relentless drive towards results</p> <p>TPMs own \"when\" something is going to get done top most, getting products out the door. Get shit done; figure it out.</p> <p>PM owns -&gt; what will the team do, what are the goals and priorities</p> <p>SDM -&gt; owns the details of how we do it</p> <p>TPM -&gt; owns making sure the top level technical plan is viable AND when we're going to deliver this thing. Get the thing out the door.</p> <p>TPMs never quit, when it gets hard/rough we find a way around it. And if it's not possible we'll figure out what is.</p> <p>Not a clipboard carrier. Not a project manager. Not a date pusher. PjM is definitely part of the job but it isn't the whole job.</p> <p>Need technical acumen to understand systems and dependencies.</p> <p>Super important to build relationships. Must be outgoing. Tribal knowledge is a fact of life, TPMs actually need to build a lot of that up to be effective. Then use playbooks and information sharing to help disseminate and share so the TK isn't hoarded (TK always exists, fact of life).</p> <p>TPM is a watermelon hunter.</p> <p>TPMs need to lead with a lot of influence. Influence comes from earning trust. Need to earn trust from all stakeholders but especially engineering. Be respectful, listen, be helpful. Not a date pusher or beat you up about a status report.</p> <p>I want to actually understand the challenges you face so I can help you AND add value. Whether it's information, or suggestion, or technology, or help. TPMs do a lot of coaching.</p> <p>Be a source of help. Full stop. Providing resources, or escalating a conflict, or bringing clarity. BE USEFUL.</p> <p>Work the non-functional requirements. Are downstream teams/systems ready for more load? Disaster recovery &amp; backups, security &amp; compliance, data residency.</p> <p>Knowing which teams will take longer or are harder to work with. TPMs know who they need to get to first; and you need to figure this out ASAP when you are new.</p> <p>Be curious. Go dig in to things yourself. Technical acumen matters, this is the T. Read the wiki, read the code, talk to an engineer and understand. Need to be capable of this.</p> <p>You need to like pulling on threads to understand how things work. Need to be curious and like to understand how things work.</p> <p>never give up, delivery matters, doing it in a timely way matters -&gt; that is the job</p> <ol> <li>be proactive</li> <li>have grit</li> <li>always speak the truth</li> <li>dont let things fester</li> <li>be resourceful</li> <li>chase things down</li> <li>relentless drive towards results</li> </ol> <p>Executives worry a lot. They deal with a lot. tl;dr of all reports is essential. Reliable and frequent communication is expected. No news is TERRIBLE news. Green things often get no followup, which is ok, but leaders will respond to Yellow and Red things and help. Exec's job is to deliver 100 different things in their portfolio.</p> <p>When you encounter something unachievable, you work to reset the expectations.</p> <p>Work with product to shape the work and give the upfront insights. Take the product plan and help break it down into a project plan along with eng -&gt; find the dependencies, the rough spots, the unknowns -&gt; then action on them and repeat</p> <p>Track all the things, hold people accountable, make sure momentum is sustained</p> <p>Ask lots of questions, probe when someone raises a concern and make sure the details are captured and not hand wavy concerns or FUD</p>"},{"location":"computers/tpm/#setting-up-a-tpm-org","title":"Setting up a TPM org","text":"<p>Visibility across the org on priorities and roadmap. Keep teams organized, keep product honest. Help get frameworks in place.</p> <p>Status reporting of work to be delivered. Create awareness, dig into the Reds and Yellows, share info and ensure dependencies and stakeholders are informed and aligned.</p> <p>Celebrate the wins, keep teams functional and happy. Recommend process improvements, get involved in Agile ceremonies as needed. But scale self and empower engineering teams to own their own process.</p> <p>Quarterly planning process in place with rigor.</p> <p>Measuring impact in place with rigor.</p> <p>Work the most critical programs for leadership. Put out the fires, ensure the biggest and riskiest programs are successful.</p> <p>Coordinate huge bodies of work that span CapOne Software and other parts of CapOne.</p> <p>Status reporting, rolling up to executives so they get R/Y/G info and summaries. Flag early and often when help is required, do not let things fester.</p> <p>DACIs and decision register to capture details and create artifacts for reference.</p> <p>Every project needs a plan. The spec is not the plan.</p> <p>Make sure you're not leading death marches. Reset expectations as needed. Present tradeoffs: you can fix this with more resources or by focusing on problem X first and moving the date out to Y.</p> <p>Coaching, TPMs do lots of coaching.</p>"},{"location":"computers/tpm/#non-functional-requirements","title":"Non-functional requirements","text":"<p>Big part of the job is picking apart the non-functional requirements and ensuring what is delivered can function and scale.</p> <p>tl;dr Fault tolerance, performance, &amp; SLIs</p> <ul> <li>Availability -&gt; percentage of time service/infra is operating normally</li> <li>Reliability -&gt; MTBR and MTTR, frequency and impact of failures</li> <li>Scalability -&gt; can the system handle an increasing workload without compromising performance</li> <li>Maintainability -&gt; operability (smooth operations under normal conditions), lucidity (simplicity of codebase), modifiability (capable of expanding without rewriting it)</li> <li>Disaster recovery -&gt; RTO &amp; RPO</li> <li>Data &amp; storage</li> </ul>"},{"location":"recipes/banana-bread/","title":"Banana bread","text":"<p>See also pumpkin bread</p>"},{"location":"recipes/banana-bread/#ingredients","title":"Ingredients","text":"<ul> <li>1 stick of softened butter (1/2 cup)</li> <li>2 cups of all purpose flour, you can also swap 1/2 cup or 1 cup whole wheat flour for a heartier loaf</li> <li>1/2 tsp salt</li> <li>1 1/2 tsp baking powder</li> <li>1/2 cup dark brown sugar</li> <li>1/2 cup white sugar</li> <li>4 big bananas that are very ripe and mash easily</li> <li>2 eggs</li> <li>1 tsp vanilla extract</li> <li>3/4 tsp nutmeg</li> <li>3/4 tsp cinnamon</li> <li>1/2 cup chopped walnuts (optional)</li> </ul>"},{"location":"recipes/banana-bread/#recipe","title":"Recipe","text":"<ol> <li>Pre-heat oven to 375F</li> <li>Grease a loaf pan with butter, I recommend one of these</li> <li>Whisk together the flour, salt, baking powder, nutmeg, and cinnamon in a medium bowl</li> <li>Cream the butter and sugars together in a large bowl until light and fluffy</li> <li>Adds eggs one at a time, followed by vanilla, mixing on low speed</li> <li>Add bananas and mix on low until just chunky bits remain</li> <li>Stir the dry ingredients in until just combined, do not over mix it will be chunky</li> <li>Gently fold in walnuts if you are using them</li> <li>Pour into loaf pan (sprinkle some brown sugar on top!) and bake abut 60 minutes or until golden brown and tester comes out almost entirely clean</li> <li>Cool in pan on a wire rack, or similar, for 15 minutes</li> <li>Then turn pan upside gently and release the loaf on top of a cutting board</li> <li>Flip it right side up, try to be patient it probably needs at least another 15min to cool/set in the middle</li> </ol> <p>Serve warm or at room temperature, keep covered/sealed at room temperature. Add sprinkles or chocolate chips inside for a fun twist.</p>"},{"location":"recipes/banana-bread/#notes","title":"Notes","text":"<ul> <li>4 large bananas works best after lots of experimentation :) But ultimately use what you have!</li> <li>1/2 tsp of nutmeg + cinnamon if you want more banana flavor to come out, or even omit entirely if you want.</li> </ul>"},{"location":"recipes/chocolate-chip-cookies/","title":"Chocolate chip cookies","text":"<p>Bakes 18 large cookies or 24 regular cookies</p>"},{"location":"recipes/chocolate-chip-cookies/#ingredients","title":"Ingredients","text":"<ul> <li>2 cups all purpose flour</li> <li>1 tsp salt</li> <li>3/4 tsp baking soda</li> <li>2 sticks of unsalted butter (i.e. 1 cup) at room temp/softened</li> <li>1 cup of dark brown sugar (light is okay)</li> <li>1/2 cup white/granular sugar</li> <li>2 eggs</li> <li>1 tsp vanilla extract</li> <li>1 and 1/3 cup dark chocolate chips or chunks</li> </ul>"},{"location":"recipes/chocolate-chip-cookies/#recipe","title":"Recipe","text":"<p>I recommend a big 21in baking sheet and parchment paper.</p> <ol> <li>Combine flour, salt, baking soda with a whisk in medium bowl.</li> <li>Cream butter &amp; sugar using a mixer until light and fluffy in a large bowl.</li> <li>Mix the eggs one at a time into the butter &amp; sugar. Then mix in the vanilla extract.</li> <li>Mix the dry ingredients into the egg/butter/sugar mixture. Slowly add the flour mixture in, about 1/3 the bowl at a time as you combine.</li> <li>Fold in the chocolate chips with a wooden spoon.</li> <li>Chill the dough for at least 20min in the freezer, it will be too sticky to work with warm. Pre-heat the oven to 375F as the dough is chilling.</li> <li>Fill your large baking sheet with 9 big hunks of dough and flatten them out slightly so its more of a disc and less of a ball. Bake them 11-12min. Or if you prefer regular sized cookies (i.e. 12 on a sheet), you will want to bake them closer to 10min. Keep an eye towards the end, ideally you want to pull them out when they are getting brown, and starting to firm, on the edges but slightly underdone in the middle still (this is the secret).</li> <li>Leave them on the pan another 5 min, this will firm them up a bit more.</li> <li>Transfer them to a wire rack to cool another 30 min at least. Patience here is key, they will be too soft to eat right away. But if you sneak a couple while they are still warm, I won't tell anyone.</li> </ol> <p>They hold up well for several days if kept air tight.</p>"},{"location":"recipes/cookie-cake/","title":"Cookie cake","text":"<p>Yum, todo</p> <p>425 15-20 min (closer to 18ish I think it was) Last time I let it cool in the cast iron for like probably an hour, mostly by accident it. But came out good, it was still very warm and fragile in the center, it could have probably sat longer! Crust ended up being good.</p>"},{"location":"recipes/donuts-baked/","title":"Baked Donuts","text":"<p>Ingredients:</p> <ul> <li>Sugar - \u00bd cup granulated sugar.</li> <li>Brown Sugar - \u2153 cup, packed.</li> <li>Butter - \u00bc cup of salted butter at room temperature.</li> <li>Cooking Oil - \u00bc cup cooking oil.</li> <li>Eggs - 2 large eggs, beaten, and at room temperature.</li> <li>Vanilla Extract - 1 \u00bd teaspoons, up to 2 teaspoons.</li> <li>Baking Powder - 2 teaspoons.</li> <li>Salt - \u00bd teaspoons. 1 teaspoon if using unsalted butter.</li> <li>All-Purpose Flour - 2 \u00bd cups, spoon and leveled.</li> <li>Nutmeg - \u00bd teaspoon.</li> <li>Milk - 1 cup of milk or buttermilk, at room temperature.</li> </ul> <p>MAKE THE DONUT BATTER</p> <ul> <li>Prep. Preheat your oven to 425\u00b0F (218\u00b0C) and lightly grease your donut pans or mini-donut pans.</li> <li>Combine dry ingredients together with a whisk in a medium bowl. Add 2 \u00bd cups of all-purpose flour, 2 teaspoons baking powder, \u00bd teaspon of salt, and \u00bd teaspon of nutmeg.</li> <li>Make the batter. In a medium-sized mixing bowl, add the room temperature \u00bc cup softened butter, \u00bc cup cooking oil, \u00bd cup sugar, and \u2153 cup brown sugar. Cream together the wet ingredients, then add the 2 large beaten eggs and 1 \u00bd teaspoons of vanilla extract. Mix until evenly distributed.</li> <li>Add the combined dry ingredients and 1 cup of milk into the wet ingredients, alternating and adding each in gradual amounts. You want the batter thoroughly combined, but do not over-mix once it comes together.</li> </ul> <p>FILL DONUT PAN &amp; BAKE Fill the donut pans. Transfer the batter into your prepared donut pans, filling each donut cavity approximately \u2154 full. Bake. Place the donut pans onto the center rack in your preheated oven and bake for 8-10 minutes until they have risen, and are golden on the bottom (you can see the lightly browned edges). Cool. Remove the baked donuts from your oven and leave them in the pan for 5 minutes before flipping them out onto a wire cooling rack. Add toppings. Add your desired sugar, cinnamon sugar, icing, and any other toppings once cooled.</p> <p>credit</p>"},{"location":"recipes/fluffy-cookie-cake/","title":"Fluffy Chocolate Chip Cookie Cake","text":""},{"location":"recipes/fluffy-cookie-cake/#ingredients","title":"Ingredients:","text":"<ul> <li>2 3/4 cups all-purpose flour</li> <li>1 teaspoon baking soda</li> <li>1/2 teaspoon baking powder</li> <li>1/2 teaspoon salt</li> <li>1 cup unsalted butter, softened</li> <li>1 cup granulated sugar</li> <li>1/2 cup brown sugar, packed</li> <li>2 large eggs</li> <li>2 teaspoons vanilla extract</li> <li>1 cup milk</li> <li>2 cups dark chocolate chips</li> </ul> <p>(feeling funky? swap out some of the choc chips for white chocolate chips!)</p>"},{"location":"recipes/fluffy-cookie-cake/#instructions","title":"Instructions:","text":"<ol> <li> <p>Preheat your oven to 350\u00b0F. Grease a 9x13 inch rectangular sheet cake pan.</p> </li> <li> <p>In a medium bowl, whisk together the flour, baking soda, baking powder, and salt. Set aside.</p> </li> <li> <p>In a large bowl or stand mixer, cream together the softened butter, granulated sugar, and brown sugar until light and fluffy, about 3-4 minutes.</p> </li> <li> <p>Beat in the eggs one at a time, then stir in the vanilla extract.</p> </li> <li> <p>Gradually add the dry ingredients to the butter mixture, alternating with the milk. Mix until just combined.</p> </li> <li> <p>Fold in 1 1/2 cups of the dark chocolate chips, reserving the remaining 1/2 cup for topping.</p> </li> <li> <p>If using white chocolate chips, or something else to jazz it up, toss in 1/2 cup of them here and then 1 cup of dark chocolate chips.</p> </li> <li> <p>Pour the batter into the prepared sheet cake pan, spreading it evenly. Sprinkle the remaining 1/2 cup of chocolate chips on top.</p> </li> <li> <p>Bake for 25-30 minutes, or until a toothpick inserted all the way to the bottom into the center comes out clean or with a few moist crumbs.</p> </li> <li> <p>Allow the cake to cool in the pan for about 15 minutes before serving.</p> </li> </ol>"},{"location":"recipes/islas-chocolate-cake/","title":"Isla's Chocolate Cake","text":"<p>Made for Isla's 3rd bday</p> <ul> <li>9x13 sheet cake</li> <li>Kitchenaid with scraper paddle for the frosting</li> </ul>"},{"location":"recipes/islas-chocolate-cake/#cake","title":"Cake","text":""},{"location":"recipes/islas-chocolate-cake/#ingredients","title":"Ingredients","text":"<p>Dry:</p> <ul> <li>2 cups flour</li> <li>2 cups sugar</li> <li>3/4 cup unsweetened cocoa powder</li> <li>2 tsp baking powder</li> <li>1 1/2 tsp baking soda</li> <li>1 tsp salt</li> </ul> <p>Wet:</p> <ul> <li>1 cup of buttermilk</li> <li>1/2 cup of vegetable oil</li> <li>2 large eggs</li> <li>2 tsp vanilla extract</li> <li>1 cup of boiling water or coffee</li> </ul>"},{"location":"recipes/islas-chocolate-cake/#instructions","title":"Instructions","text":"<ol> <li>Preheat oven to 350\u00b0F. Grease pan.</li> <li>Whisk together flour, sugar, cocoa powder, baking powder, baking soda, and salt.</li> <li>Mix buttermilk, oil, eggs, and vanilla. Beat until well combined.</li> <li>Combine wet + dry.</li> <li>Slowly mix in hot water or coffee (batter will be thin, but that\u2019s normal).</li> <li>Bake for 30-35 minutes, or until a toothpick comes out clean.</li> <li>Let cake cool in pan completely before adding frosting.</li> </ol>"},{"location":"recipes/islas-chocolate-cake/#frosting","title":"Frosting","text":""},{"location":"recipes/islas-chocolate-cake/#ingredients_1","title":"Ingredients","text":"<ul> <li>3/4 cup unsalted butter, softened</li> <li>3 cups powdered sugar</li> <li>2 tsp vanilla extract</li> <li>3 tbsp heavy cream or milk</li> <li>Pinch of salt</li> </ul>"},{"location":"recipes/islas-chocolate-cake/#instructions_1","title":"Instructions","text":"<ol> <li>Use stand mixer with the paddle attachment (flat beater if available). The paddle gives the frosting a smooth, creamy texture without incorporating too much air, which is ideal for spreading on a sheet cake.</li> <li>Beat the butter for 2 minutes until pale and fluffy.</li> <li>Add powdered sugar, half cup at a time, mixing on low at first, then increasing speed.</li> <li>Add vanilla and 3 tbsp of cream/milk. Mix on medium-high speed for 2-3 minutes until fluffy.</li> <li>Spread evenly over the completely cooled sheet cake.</li> </ol>"},{"location":"recipes/pancakes/","title":"Pancakes","text":"<ul> <li>2 cup flour</li> <li>2 tsp baking powder</li> <li>1/2 tsp salt</li> <li>1 tbsp sugar (optional)</li> <li>2 tbsp butter melted &amp; cooled (optional)</li> <li>3 eggs separated</li> <li> <p>1 cup of milk</p> </li> <li> <p>Mix dry ingredients</p> </li> <li>Combine wet</li> <li>Beat egg whites</li> <li>Combine wet + egg whites</li> <li>Combine all</li> </ul>"},{"location":"recipes/pecan-pie/","title":"Pecan pie","text":""},{"location":"recipes/pecan-pie/#ingredients","title":"Ingredients","text":"<ul> <li>1/3 cup butter, melted</li> <li>1 cup packed dark brown sugar</li> <li>3/4 cup dark corn syrup</li> <li>1/2 teaspoon salt (optional if using salted butter)</li> <li>3 eggs</li> <li>1 teaspoon vanilla extract (optional)</li> <li>1 tablespoon of bourbon (optional)</li> <li>8oz bag (or 2 cups) of pecans, halves or pieces</li> <li>1 pie crust, refrigerated, frozen, or your own</li> <li>Ice cream or Cool Whip to go with it (optional)</li> </ul>"},{"location":"recipes/pecan-pie/#hardware","title":"Hardware","text":"<ul> <li>9in pie pan with a depth of 1 to 1.5in works best but bigger is ok</li> <li>Large bowl for mixing, hand/stand mixer optional but recommended</li> <li>Measuring cups</li> <li>Aluminum foil</li> </ul>"},{"location":"recipes/pecan-pie/#recipe","title":"Recipe","text":"<ol> <li>Pre-heat oven to 375F conventional bake.</li> <li>Place room temperature pie crust inside pie pan.</li> <li>Put half (or 1 cup) of the pecans on the bottom of the crust. Don't obsess over the amount, you just want to cover most of the bottom of the crust with a layer of pecans.</li> <li>Melt the 1/3 cup butter. While it's melting, you can move on to the next step if you'd like to starting getting the other ingredients prepared and into the mixing bowl (don't combine until the butter is ready though).</li> <li>Mix together melted butter, 1 cup packed dark brown sugar, 3/4 cup dark corn syrup, 1/2 teaspoon salt, 3 eggs, 1 teaspoon vanilla extract, 1 tablespoon of bourbon. Blend well, hand or stand mixer recommended. The mixture should be pretty thin and runny, smooth with few, if any, lumps, and brown in color.</li> <li>Pour the mixture into the pie crust.</li> <li>Distribute the remaining pecans on the top. If you want to get fancy, you can arrange them in a pattern for a really nice look. Try turning them all in the same direction, for example.</li> <li>Bake for 15-20 minutes, checking at the 15min mark. When the crust is starting to turn a light golden brown color, you want to pull it out and cover the crust/edges in foil so they don't burn. Keep your oven closed to keep the heat in, and be careful as to not let the pie splash around when you pull it out as it will be very liquid still. To cover the edges, tear off a good size sheet of aluminum foil into four strips you can place around the pie crust. Don't worry if it's not perfectly on there, even just resting the strips on top in a square shape works fine to keep the crust from burning.</li> <li>Bake for another 15-20 minutes or until the center of the pie is set. What you are looking for is no or barely any movement at all from your pie if you move the pan around. While it's still cooking, you will notice the center will jiggle around and be fluid. The longer it cooks the more solid it will get, you want to pull it out once it's set and no longer moving.</li> <li>Cool for at least 30 minutes.</li> <li>Now the hard part. You can serve it warm, buttttt it tastes best if you refrigerate about two hours and then bring it back to room temperature before serving. Enjoying with cool whip, ice cream, and/or friends is highly recommended as well.</li> </ol>"},{"location":"recipes/pizza/","title":"Pizza","text":""},{"location":"recipes/pizza/#dough","title":"Dough","text":"<p>Ingredients:</p> <ul> <li>2 1/4 cups all-purpose flour (plus extra for dusting)</li> <li>1 tsp sugar</li> <li>1 tsp salt</li> <li>1 tsp of oregano (optional)</li> <li>1 tsp of garlic powder (optional)</li> <li>1 tbsp olive oil (plus extra for coating)</li> <li>1 tsp active dry yeast (or you can use instant)</li> <li>3/4 cup warm water (about 110\u00b0F, just not crazy hot or it will kill the yeast)</li> </ul> <p>Instructions:</p> <ol> <li>Activate the Yeast: In a small bowl, dissolve sugar in warm water. Sprinkle the yeast over the top. Let it sit for about 5 minutes, until it becomes frothy.</li> <li>Mix Dry Ingredients: In a large bowl, mix together the flour, salt, oregano, and garlic powder (if using).</li> <li>Combine: Once the yeast mixture is ready, add it along with the olive oil to the flour mixture.</li> <li>Knead the Dough: On a lightly floured surface, knead the dough for about 10 minutes until it becomes smooth and elastic. This step is crucial for developing the gluten, which gives the dough its texture.</li> <li>Let it Rise: Place the dough in a lightly oiled bowl, cover with a cloth, and let it rise in a warm area for at least 1 hour (2-3 is better if you have the time). It should at least double in size from when you placed it in the bowl.</li> <li>Punch Down &amp; Shape: Once risen, punch down the dough to release air bubbles. Divide it into portions if you want smaller pizzas, and shape it into your desired size.</li> <li>Final Rise: Let the shaped dough rest for an additional 30 minutes.</li> </ol> <p>Preheat Oven: Meanwhile, preheat your oven to 425\u00b0F to 475\u00b0F. If you have a pizza stone, place it in the oven during preheating.</p> <p>Top &amp; Bake: Add your favorite toppings and bake for 10-15 minutes until the crust is golden and the cheese is bubbly.</p> <p>Tips:</p> <ul> <li>Cold Ferment for Flavor: For an even better flavor, let the dough cold ferment in the refrigerator for 24 to 72 hours after the first rise.</li> <li>Hydration Matters: The water-to-flour ratio (hydration) can greatly affect the texture. This recipe is about 65% hydration, which is a good balance for most home ovens.</li> </ul>"},{"location":"recipes/pumpkin-bread/","title":"Pumpkin bread","text":"<p>See also banana bread</p>"},{"location":"recipes/pumpkin-bread/#ingredients","title":"Ingredients","text":"<ul> <li>1 stick of butter (1/2 cup) softened</li> <li>2 cups of all purpose flour, you can also swap 1/2 cup or 1 cup whole wheat flour for a heartier loaf</li> <li>1/2 tsp salt</li> <li>1 1/2 tsp baking powder</li> <li>1 cup dark brown sugar (white/granular is okay)</li> <li>1 cup of pumpkin puree</li> <li>2 eggs</li> <li>1 tsp vanilla extract</li> <li>1 tsp nutmeg</li> <li>1 tsp cinnamon</li> <li>1/2 cup chopped walnuts</li> </ul>"},{"location":"recipes/pumpkin-bread/#recipe","title":"Recipe","text":"<ol> <li>Pre-heat oven to 350F</li> <li>Grease a loaf pan with butter, I recommend one of these</li> <li>Whisk together the flour, salt, baking powder, and sugar in a medium bowl</li> <li>Mix the pumpkin and the butter until butter is broken up and mixture is chunky but starting to smooth</li> <li>Mix the eggs and vanilla into the pumpkin butter mixture until well combined</li> <li>Stir the dry ingredients in until just combined, do not over mix it will be chunky</li> <li>Gently fold in walnuts if you are using them</li> <li>Pour into loaf pan and bake 50-60 minutes until golden brown and toothpick comes out almost entirely clean</li> <li>Cool in pan on a wire rack for 15 minutes</li> <li>Then turn pan upside gently and release the loaf on top of a cutting board</li> <li>Flip it rightside up and serve warm or at room temperature</li> </ol> <p>Keep covered/sealed at room temperature</p>"},{"location":"recipes/strawberry-cupcakes/","title":"Strawberry cupcakes","text":"<p>todo</p>"},{"location":"recipes/vanilla-cupcakes/","title":"Vanilla cupcakes","text":"<p>todo</p>"},{"location":"seattle/kids/","title":"Things to do with kids","text":"<ul> <li>Seattle Parks &amp; Rec</li> <li>PEPS List lots of good stuff in here</li> </ul>"},{"location":"seattle/kids/#indoors","title":"Indoors","text":"<ul> <li>REI Flagship store</li> <li>PlayDate SEA (indoor play place SLU)</li> <li>Outer Space Seattle (indoor play place Alki)</li> <li>Jungle Gym</li> <li>South center mall</li> <li>Bellevue mall</li> <li>Indoor tot gyms Seattle parks &amp; rec</li> <li>Space Needle</li> <li>Kraken practice arena free skate</li> <li>Planetarium at Seattle Center</li> <li>YMCA Kid Zone</li> <li>Climbing gym -&gt; Bouldering Project</li> <li>Seattle Aquarium</li> <li>Zoomazium play area inside Woodland Park Zoo</li> <li>Ice skating at (Kraken Community Iceplex)[https://www.krakencommunityiceplex.com/] in Northgate or any (Sno-King)[https://www.snokingicearenas.com/] location (Renton is good)</li> <li>(Southgate Roller Rink)[http://www.southgaterollerrink.com/]</li> </ul>"},{"location":"seattle/kids/#outdoors","title":"Outdoors","text":"<ul> <li>Arboretum, South Seattle college</li> <li>West Seattle Stadium bike riding and walking</li> <li>Pretty much any community center playground, like Highpoint</li> <li>Alki beach</li> <li>Lincoln Park</li> <li>Hiawatha playground/park + Good Society for beer and pizza after</li> <li>Seattle Center including playground with gigantic slide</li> <li>Water taxi across the sound to downtown</li> <li>White Center bike playground Dick Thornau Memorial Park</li> <li>Kelsey Creek Farm in Bellevue, they have farm classes for the kids too!</li> <li>Schmitz preserve, walk trail down to Alki and take the bus or free water taxi shuttle bus back up the hill</li> <li>Sprayparks and wading pools</li> <li>Woodland Park Zoo</li> <li>Sno-parks for playing in the snow and sledding, way cheaper than tubing at ski resorts</li> <li>they all require a Discover Pass + Sno-park day pass</li> <li>Lake Wenatchee state park sno-park is the best one, only good if staying overnight in Leavenworth because it's so far though.</li> <li>Hyak sno-park is great but busiest (closest to Seattle), Lake Easton is good too and a couple exits further</li> <li>Asahel Curtis sno-park</li> <li>Cool parks a little bit away from West Seattle for a change of scenery</li> <li>Lake Sammamish State Park</li> <li>Pioneer Park in Puyallup</li> <li>Carkeek Park</li> <li>Dragon park Mercer Island</li> <li>Volunteer Park Capitol Hill</li> </ul>"},{"location":"seattle/lawn/","title":"Lawn care","text":"<ul> <li>Seattle Public Utilities natural lawn care</li> <li>Big ol PDF of the above</li> <li>Ecologically sound lawn care for the PNW</li> <li>Sustainable lawn care for the PWN</li> <li>Integrated pest control</li> </ul>"},{"location":"seattle/lawn/#spring","title":"Spring","text":"<p>April - May</p> <ul> <li>Mow regularly at 2-3\u201d, leave the clippings for free fertilizer</li> <li>If needed, fertilize mid-May</li> <li>Improve thin areas with aeration, overseeding, and topdressing with compost</li> <li>Pull weeds before they seed</li> </ul>"},{"location":"seattle/lawn/#summer","title":"Summer","text":"<p>June - August</p> <ul> <li>Water deeply, 1 inch per week, or let lawn go golden and dormant</li> <li>Mow less often as growth slows</li> </ul>"},{"location":"seattle/lawn/#fall","title":"Fall","text":"<p>September - November</p> <ul> <li>On thin areas, overseed and topdress with compost in Sept.</li> <li>If needed, apply organic fertilizer September-October, or slow-release synthetic until November</li> <li>Pull fall weeds before they seed</li> <li>Mow lower (1\u201d) the last time in fall</li> </ul>"},{"location":"seattle/lawn/#winter","title":"Winter","text":"<p>December - March</p> <ul> <li>Get mower blades sharpened before the spring rush</li> <li>Test soil once every 3 years to plan fertilizer and lime needs</li> </ul>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/computers/","title":"computers","text":""}]}